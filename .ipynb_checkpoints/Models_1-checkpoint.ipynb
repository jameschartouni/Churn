{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#james chartouni\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"cleaned_input/train_consolidated.csv\")\n",
    "training_data = training_data.drop(['msno'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"cleaned_input/test_consolidated.csv\")\n",
    "#test_data = test_data.drop(['msno'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#split data \n",
    "y = training_data[\"is_churn\"].values\n",
    "X = training_data.drop([\"is_churn\"], axis=1).values\n",
    "#replaced infinite values with zero. MAKE SURE THIS IS the right thing to do \n",
    "X[X == -inf] = 0\n",
    "X[X == inf] = 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -0.19 (+/- 0.1852)\n"
     ]
    }
   ],
   "source": [
    "#logistic regression \n",
    "logistic_pipe = make_pipeline(LogisticRegression(solver='liblinear'))\n",
    "scores = cross_val_score(logistic_pipe, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "#logistic regression w/ polynomial features 2   \n",
    "logistic_pipe = make_pipeline(PolynomialFeatures(degree=2), LogisticRegression(solver='liblinear'))\n",
    "scores = cross_val_score(logistic_pipe, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "#logistic regression w/th PCA, Polynomial features \n",
    "logistic_pipe = make_pipeline(PCA(n_components=.95), PolynomialFeatures(degree=2), LogisticRegression(solver='liblinear'))\n",
    "scores = cross_val_score(logistic_pipe, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "#Light GBM \n",
    "# create dataset for lightgbm\n",
    "# if you want to re-use data, remember to set free_raw_data=False\n",
    "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=False)\n",
    "\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'njobs': -1\n",
    "}\n",
    "# generate a feature name\n",
    "feature_name = X_train.dtype.names\n",
    "\n",
    "#cross validation\n",
    "num_round = 50\n",
    "scores = lgb.cv(params, lgb_train, num_round, nfold=5, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print('Start training...')\n",
    "# feature_name and categorical_feature\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=50,\n",
    "                valid_sets=lgb_train,  # eval training data\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "print(\"--------------------------------------------------------------\")\n",
    "ypred = gbm.predict(X_test)\n",
    "print(log_loss(y_test, ypred))\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "# feature names\n",
    "print('Feature names:', gbm.feature_name())\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importance()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def xgb_score(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'log_loss', metrics.log_loss(labels, preds)\n",
    "\n",
    "fold = 1\n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        'eta': 0.02, #use 0.002\n",
    "        'max_depth': 7,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'seed': i,\n",
    "        'silent': False\n",
    "    }\n",
    "    watchlist = [(xgb.DMatrix(X_train, y_train), 'train'), (xgb.DMatrix(X_test, y_test), 'valid')]\n",
    "    model = xgb.train(params, xgb.DMatrix(X_train, y_train), 150,  watchlist, feval=xgb_score, maximize=False, verbose_eval=50, early_stopping_rounds=50) #use 1500\n",
    "    if i != 0:\n",
    "        pred += model.predict(xgb.DMatrix(test[cols]), ntree_limit=model.best_ntree_limit)\n",
    "    else:\n",
    "        pred = model.predict(xgb.DMatrix(test[cols]), ntree_limit=model.best_ntree_limit)\n",
    "pred /= fold\n",
    "test['is_churn'] = pred.clip(0.0000001, 0.999999)\n",
    "test[['msno','is_churn']].to_csv('submission3.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
