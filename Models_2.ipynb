{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#james chartouni\n",
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import inf\n",
        "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures\n",
        "from sklearn.metrics import log_loss"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = pd.read_csv(\"cleaned_input/train_consolidated.csv\")\n",
        "training_data = training_data.drop(['msno'], axis=1)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split data \n",
        "y = training_data[\"is_churn\"].values\n",
        "X = training_data.drop([\"is_churn\"], axis=1).values\n",
        "#replaced infinite values with zero. MAKE SURE THIS IS the right thing to do \n",
        "X[X == -inf] = 0\n",
        "X[X == inf] = 0\n",
        "\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression \n",
        "logistic_pipe = make_pipeline(LogisticRegression(solver='liblinear', n_jobs=-1))\n",
        "scores = cross_val_score(logistic_pipe, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
        "print(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
        "\n",
        "#logistic regression w/ polynomial features 2   \n",
        "logistic_pipe = make_pipeline(PolynomialFeatures(degree=2), LogisticRegression(solver='liblinear', n_jobs=-1))\n",
        "scores = cross_val_score(logistic_pipe, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
        "print(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
        "\n",
        "#logistic regression w/ polynomial features 3  \n",
        "logistic_pipe = make_pipeline(PolynomialFeatures(degree=3), LogisticRegression(solver='liblinear', n_jobs=-1))\n",
        "scores = cross_val_score(logistic_pipe, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
        "print(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
        "\n",
        "#logistic regression w/th PCA, Polynomial features \n",
        "logistic_pipe = make_pipeline(PCA(n_components=.95), PolynomialFeatures(degree=3), LogisticRegression(solver='liblinear', n_jobs=-1))\n",
        "scores = cross_val_score(logistic_pipe, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
        "print(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
        "\n",
        "#logistic regression \n",
        "logistic_pipe = make_pipeline(LinearDiscriminantAnalysis(), LogisticRegression(solver='liblinear', n_jobs=-1))\n",
        "scores = cross_val_score(logistic_pipe, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
        "print(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
        "\n",
        "#logistic regression \n",
        "logistic_pipe = make_pipeline(QuadraticDiscriminantAnalysis(), LogisticRegression(solver='liblinear', n_jobs=-1))\n",
        "scores = cross_val_score(logistic_pipe, X_train, y_train, cv=5, scoring=\"neg_log_loss\")\n",
        "print(\"Accuracy: %0.2f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/James/anaconda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/Users/James/anaconda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/Users/James/anaconda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/Users/James/anaconda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/Users/James/anaconda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/Users/James/anaconda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/Users/James/anaconda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: -0.56 (+/- 0.5453)\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Light GBM \n",
        "# create dataset for lightgbm\n",
        "# if you want to re-use data, remember to set free_raw_data=False\n",
        "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,free_raw_data=False)\n",
        "\n\n\n\n",
        "# specify your configurations as a dict\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0,\n",
        "    'njobs': -1\n",
        "}\n",
        "# generate a feature name\n",
        "feature_name = X_train.dtype.names\n",
        "\n",
        "#cross validation\n",
        "num_round = 50\n",
        "scores = lgb.cv(params, lgb_train, num_round, nfold=5, verbose_eval=True)\n",
        "\n\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\tcv_agg's binary_logloss: 0.644426 + 2.18444e-06\n",
            "[2]\tcv_agg's binary_logloss: 0.600342 + 4.42969e-06\n",
            "[3]\tcv_agg's binary_logloss: 0.56026 + 6.61193e-06\n",
            "[4]\tcv_agg's binary_logloss: 0.523668 + 8.77079e-06\n",
            "[5]\tcv_agg's binary_logloss: 0.490143 + 8.83851e-06\n",
            "[6]\tcv_agg's binary_logloss: 0.45933 + 1.00787e-05\n",
            "[7]\tcv_agg's binary_logloss: 0.433173 + 1.1764e-05\n",
            "[8]\tcv_agg's binary_logloss: 0.409016 + 1.76675e-05\n",
            "[9]\tcv_agg's binary_logloss: 0.384395 + 1.68195e-05\n",
            "[10]\tcv_agg's binary_logloss: 0.361561 + 1.68567e-05\n",
            "[11]\tcv_agg's binary_logloss: 0.340343 + 1.6456e-05\n",
            "[12]\tcv_agg's binary_logloss: 0.320596 + 1.71062e-05\n",
            "[13]\tcv_agg's binary_logloss: 0.302189 + 1.76867e-05\n",
            "[14]\tcv_agg's binary_logloss: 0.285009 + 1.86381e-05\n",
            "[15]\tcv_agg's binary_logloss: 0.26895 + 1.86471e-05\n",
            "[16]\tcv_agg's binary_logloss: 0.253926 + 1.97934e-05\n",
            "[17]\tcv_agg's binary_logloss: 0.239855 + 2.10112e-05\n",
            "[18]\tcv_agg's binary_logloss: 0.226661 + 2.19625e-05\n",
            "[19]\tcv_agg's binary_logloss: 0.214279 + 2.29504e-05\n",
            "[20]\tcv_agg's binary_logloss: 0.202648 + 2.28154e-05\n",
            "[21]\tcv_agg's binary_logloss: 0.191717 + 2.20308e-05\n",
            "[22]\tcv_agg's binary_logloss: 0.181434 + 2.20543e-05\n",
            "[23]\tcv_agg's binary_logloss: 0.171757 + 2.2912e-05\n",
            "[24]\tcv_agg's binary_logloss: 0.162642 + 2.26434e-05\n",
            "[25]\tcv_agg's binary_logloss: 0.154053 + 2.20036e-05\n",
            "[26]\tcv_agg's binary_logloss: 0.145954 + 2.14013e-05\n",
            "[27]\tcv_agg's binary_logloss: 0.138314 + 2.09874e-05\n",
            "[28]\tcv_agg's binary_logloss: 0.131103 + 2.03463e-05\n",
            "[29]\tcv_agg's binary_logloss: 0.124293 + 1.95871e-05\n",
            "[30]\tcv_agg's binary_logloss: 0.117862 + 1.95833e-05\n",
            "[31]\tcv_agg's binary_logloss: 0.111785 + 1.96906e-05\n",
            "[32]\tcv_agg's binary_logloss: 0.106041 + 1.99604e-05\n",
            "[33]\tcv_agg's binary_logloss: 0.100609 + 2.04394e-05\n",
            "[34]\tcv_agg's binary_logloss: 0.0959595 + 1.9477e-05\n",
            "[35]\tcv_agg's binary_logloss: 0.0910696 + 2.00956e-05\n",
            "[36]\tcv_agg's binary_logloss: 0.0864419 + 1.89725e-05\n",
            "[37]\tcv_agg's binary_logloss: 0.0820614 + 1.92142e-05\n",
            "[38]\tcv_agg's binary_logloss: 0.0779138 + 1.85195e-05\n",
            "[39]\tcv_agg's binary_logloss: 0.073985 + 1.87767e-05\n",
            "[40]\tcv_agg's binary_logloss: 0.0702626 + 1.84521e-05\n",
            "[41]\tcv_agg's binary_logloss: 0.066735 + 1.82922e-05\n",
            "[42]\tcv_agg's binary_logloss: 0.063393 + 1.89125e-05\n",
            "[43]\tcv_agg's binary_logloss: 0.0602237 + 1.84296e-05\n",
            "[44]\tcv_agg's binary_logloss: 0.0572189 + 1.84549e-05\n",
            "[45]\tcv_agg's binary_logloss: 0.0543698 + 1.89238e-05\n",
            "[46]\tcv_agg's binary_logloss: 0.0516666 + 1.91629e-05\n",
            "[47]\tcv_agg's binary_logloss: 0.0491023 + 1.98105e-05\n",
            "[48]\tcv_agg's binary_logloss: 0.0466692 + 2.03251e-05\n",
            "[49]\tcv_agg's binary_logloss: 0.0443605 + 2.13495e-05\n",
            "[50]\tcv_agg's binary_logloss: 0.0421707 + 2.15615e-05\n"
          ]
        }
      ],
      "execution_count": 40,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Start training...')\n",
        "# feature_name and categorical_feature\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=50,\n",
        "                valid_sets=lgb_train,  # eval training data\n",
        "              )\n",
        "\n\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "[1]\ttraining's binary_logloss: 0.644421\n",
            "[2]\ttraining's binary_logloss: 0.600332\n",
            "[3]\ttraining's binary_logloss: 0.560244\n",
            "[4]\ttraining's binary_logloss: 0.523647\n",
            "[5]\ttraining's binary_logloss: 0.490118\n",
            "[6]\ttraining's binary_logloss: 0.459299\n",
            "[7]\ttraining's binary_logloss: 0.433141\n",
            "[8]\ttraining's binary_logloss: 0.408971\n",
            "[9]\ttraining's binary_logloss: 0.384348\n",
            "[10]\ttraining's binary_logloss: 0.361511\n",
            "[11]\ttraining's binary_logloss: 0.340291\n",
            "[12]\ttraining's binary_logloss: 0.320542\n",
            "[13]\ttraining's binary_logloss: 0.302131\n",
            "[14]\ttraining's binary_logloss: 0.284948\n",
            "[15]\ttraining's binary_logloss: 0.268888\n",
            "[16]\ttraining's binary_logloss: 0.25386\n",
            "[17]\ttraining's binary_logloss: 0.239787\n",
            "[18]\ttraining's binary_logloss: 0.226588\n",
            "[19]\ttraining's binary_logloss: 0.214203\n",
            "[20]\ttraining's binary_logloss: 0.202571\n",
            "[21]\ttraining's binary_logloss: 0.191637\n",
            "[22]\ttraining's binary_logloss: 0.181352\n",
            "[23]\ttraining's binary_logloss: 0.171672\n",
            "[24]\ttraining's binary_logloss: 0.162554\n",
            "[25]\ttraining's binary_logloss: 0.153963\n",
            "[26]\ttraining's binary_logloss: 0.145861\n",
            "[27]\ttraining's binary_logloss: 0.138218\n",
            "[28]\ttraining's binary_logloss: 0.131004\n",
            "[29]\ttraining's binary_logloss: 0.124194\n",
            "[30]\ttraining's binary_logloss: 0.11776\n",
            "[31]\ttraining's binary_logloss: 0.111679\n",
            "[32]\ttraining's binary_logloss: 0.105932\n",
            "[33]\ttraining's binary_logloss: 0.100496\n",
            "[34]\ttraining's binary_logloss: 0.0958485\n",
            "[35]\ttraining's binary_logloss: 0.0909564\n",
            "[36]\ttraining's binary_logloss: 0.0863257\n",
            "[37]\ttraining's binary_logloss: 0.0819406\n",
            "[38]\ttraining's binary_logloss: 0.0777879\n",
            "[39]\ttraining's binary_logloss: 0.0738552\n",
            "[40]\ttraining's binary_logloss: 0.0701299\n",
            "[41]\ttraining's binary_logloss: 0.0665993\n",
            "[42]\ttraining's binary_logloss: 0.0632525\n",
            "[43]\ttraining's binary_logloss: 0.0600798\n",
            "[44]\ttraining's binary_logloss: 0.0570714\n",
            "[45]\ttraining's binary_logloss: 0.0542192\n",
            "[46]\ttraining's binary_logloss: 0.0515138\n",
            "[47]\ttraining's binary_logloss: 0.0489475\n",
            "[48]\ttraining's binary_logloss: 0.0465119\n",
            "[49]\ttraining's binary_logloss: 0.044201\n",
            "[50]\ttraining's binary_logloss: 0.0420083\n"
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--------------------------------------------------------------\")\n",
        "ypred = gbm.predict(X_test)\n",
        "print(log_loss(y_test, ypred))\n",
        "# save model to file\n",
        "gbm.save_model('model.txt')\n",
        "\n",
        "# feature names\n",
        "print('Feature names:', gbm.feature_name())\n",
        "\n",
        "# feature importances\n",
        "print('Feature importances:', list(gbm.feature_importance()))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------\n",
            "0.0422455120945\n",
            "Feature names: ['Column_0', 'Column_1', 'Column_2', 'Column_3', 'Column_4', 'Column_5', 'Column_6', 'Column_7', 'Column_8', 'Column_9', 'Column_10', 'Column_11', 'Column_12', 'Column_13', 'Column_14', 'Column_15', 'Column_16', 'Column_17', 'Column_18', 'Column_19', 'Column_20', 'Column_21', 'Column_22', 'Column_23', 'Column_24', 'Column_25', 'Column_26', 'Column_27', 'Column_28', 'Column_29', 'Column_30', 'Column_31', 'Column_32', 'Column_33', 'Column_34', 'Column_35', 'Column_36', 'Column_37', 'Column_38', 'Column_39', 'Column_40', 'Column_41']\n",
            "Feature importances: [94, 103, 92, 56, 62, 27, 36, 49, 24, 156, 38, 1, 17, 4, 63, 23, 15, 7, 21, 39, 20, 26, 0, 0, 32, 98, 26, 101, 17, 46, 3, 72, 0, 4, 5, 6, 27, 4, 31, 27, 14, 14]\n"
          ]
        }
      ],
      "execution_count": 43,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.3.4"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}